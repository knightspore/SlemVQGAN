{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060 Ti, 8192 MiB, 7441 MiB\n",
      "\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gputil in /home/c/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: psutil in /home/c/.local/lib/python3.10/site-packages (5.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: humanize in /home/c/.local/lib/python3.10/site-packages (4.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Wed May 18 01:59:44 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.68.02    Driver Version: 510.68.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:10:00.0  On |                  N/A |\n",
      "|  0%   34C    P8    15W / 200W |    535MiB /  8192MiB |     32%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1272      G   /usr/lib/Xorg                     175MiB |\n",
      "|    0   N/A  N/A      1471      G   picom                               3MiB |\n",
      "|    0   N/A  N/A      1906      G   ...251080353911444768,131072      277MiB |\n",
      "|    0   N/A  N/A     64473      G   alacritty                          19MiB |\n",
      "|    0   N/A  N/A     71602      G   ...RendererForSitePerProcess       56MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Gen RAM Free: 26.3 GB  |     Proc size: 63.5 MB\n",
      "GPU RAM Free: 7429MB | Used: 536MB | Util   7% | Total     8192MB\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "from subprocess import getoutput\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader\n",
    "print(\"\\n\")\n",
    "\n",
    "# Setup\n",
    "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "%pip install gputil\n",
    "%pip install psutil\n",
    "%pip install humanize\n",
    "\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "\n",
    "# Print Details\n",
    "!nvidia-smi\n",
    "\n",
    "GPUs = GPU.getGPUs()\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Deps, Load Libraries \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'CLIP' already exists and is not an empty directory.\n",
      "fatal: destination path 'taming-transformers' already exists and is not an empty directory.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ftfy in /home/c/.local/lib/python3.10/site-packages (6.1.1)\n",
      "Requirement already satisfied: regex in /home/c/.local/lib/python3.10/site-packages (2022.4.24)\n",
      "Requirement already satisfied: tqdm in /home/c/.local/lib/python3.10/site-packages (4.64.0)\n",
      "Requirement already satisfied: omegaconf in /home/c/.local/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: pytorch-lightning in /home/c/.local/lib/python3.10/site-packages (1.6.3)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/c/.local/lib/python3.10/site-packages (from ftfy) (0.2.5)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/lib/python3.10/site-packages (from omegaconf) (6.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/c/.local/lib/python3.10/site-packages (from omegaconf) (4.8)\n",
      "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /home/c/.local/lib/python3.10/site-packages (from pytorch-lightning) (0.3.2)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /home/c/.local/lib/python3.10/site-packages (from pytorch-lightning) (0.8.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/c/.local/lib/python3.10/site-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/c/.local/lib/python3.10/site-packages (from pytorch-lightning) (2.9.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /home/c/.local/lib/python3.10/site-packages (from pytorch-lightning) (2022.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/c/.local/lib/python3.10/site-packages (from pytorch-lightning) (1.22.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/c/.local/lib/python3.10/site-packages (from pytorch-lightning) (4.2.0)\n",
      "Requirement already satisfied: torch>=1.8.* in /home/c/.local/lib/python3.10/site-packages (from pytorch-lightning) (1.11.0)\n",
      "Requirement already satisfied: aiohttp in /home/c/.local/lib/python3.10/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
      "Requirement already satisfied: requests in /home/c/.local/lib/python3.10/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/c/.local/lib/python3.10/site-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/c/.local/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/c/.local/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/c/.local/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/c/.local/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.46.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/c/.local/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/c/.local/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/c/.local/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.6.6)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/c/.local/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.20.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (59.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/c/.local/lib/python3.10/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
      "Requirement already satisfied: six in /home/c/.local/lib/python3.10/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/c/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/c/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/c/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/c/.local/lib/python3.10/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/c/.local/lib/python3.10/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/c/.local/lib/python3.10/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/c/.local/lib/python3.10/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/c/.local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/c/.local/lib/python3.10/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/c/.local/lib/python3.10/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/c/.local/lib/python3.10/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/c/.local/lib/python3.10/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/c/.local/lib/python3.10/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/c/.local/lib/python3.10/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kornia in /home/c/.local/lib/python3.10/site-packages (0.6.5)\n",
      "Requirement already satisfied: torch>=1.8.1 in /home/c/.local/lib/python3.10/site-packages (from kornia) (1.11.0)\n",
      "Requirement already satisfied: packaging in /home/c/.local/lib/python3.10/site-packages (from kornia) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /home/c/.local/lib/python3.10/site-packages (from torch>=1.8.1->kornia) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/c/.local/lib/python3.10/site-packages (from packaging->kornia) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in /home/c/.local/lib/python3.10/site-packages (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/c/.local/lib/python3.10/site-packages (4.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/c/.local/lib/python3.10/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/c/.local/lib/python3.10/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/c/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.10/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/c/.local/lib/python3.10/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/c/.local/lib/python3.10/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: requests in /home/c/.local/lib/python3.10/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/c/.local/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/c/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/c/.local/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/c/.local/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/c/.local/lib/python3.10/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/c/.local/lib/python3.10/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/c/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "!git clone https://github.com/openai/CLIP\n",
    "!git clone https://github.com/CompVis/taming-transformers\n",
    "%pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
    "%pip install kornia\n",
    "%pip install einops\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries and Variables\n",
    "import argparse\n",
    "import math\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append('./taming-transformers')\n",
    "\n",
    "from IPython import display\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from taming.models import cond_transformer, vqgan\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os.path\n",
    "from os import path\n",
    "from urllib.request import Request, urlopen\n",
    " \n",
    "from CLIP import clip\n",
    "import kornia\n",
    "import kornia.augmentation as K\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "from base64 import b64encode\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_printoptions( sci_mode=False )\n",
    "\n",
    "def noise_gen(shape, octaves=5):\n",
    "    n, c, h, w = shape\n",
    "    noise = torch.zeros([n, c, 1, 1])\n",
    "    max_octaves = min(octaves, math.log(h)/math.log(2), math.log(w)/math.log(2))\n",
    "    for i in reversed(range(max_octaves)):\n",
    "        h_cur, w_cur = h // 2**i, w // 2**i\n",
    "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
    "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
    "    return noise\n",
    "\n",
    "\n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "\n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    "\n",
    "    input = input.view([n * c, 1, h, w])\n",
    "\n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    "\n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    "\n",
    "    input = input.view([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    "    \n",
    "\n",
    "# def replace_grad(fake, real):\n",
    "#     return fake.detach() - real.detach() + real\n",
    "\n",
    "\n",
    "class ReplaceGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_forward, x_backward):\n",
    "        ctx.shape = x_backward.shape\n",
    "        return x_forward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        return None, grad_in.sum_to_size(ctx.shape)\n",
    "\n",
    "\n",
    "class ClampWithGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, min, max):\n",
    "        ctx.min = min\n",
    "        ctx.max = max\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min, max)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
    "\n",
    "replace_grad = ReplaceGrad.apply\n",
    "\n",
    "clamp_with_grad = ClampWithGrad.apply\n",
    "# clamp_with_grad = torch.clamp\n",
    "\n",
    "def vector_quantize(x, codebook):\n",
    "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
    "    indices = d.argmin(-1)\n",
    "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
    "    return replace_grad(x_q, x)\n",
    "\n",
    "\n",
    "class Prompt(nn.Module):\n",
    "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
    "        super().__init__()\n",
    "        self.register_buffer('embed', embed)\n",
    "        self.register_buffer('weight', torch.as_tensor(weight))\n",
    "        self.register_buffer('stop', torch.as_tensor(stop))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        input_normed = F.normalize(input.unsqueeze(1), dim=2)#(input / input.norm(dim=-1, keepdim=True)).unsqueeze(1)# \n",
    "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)#(self.embed / self.embed.norm(dim=-1, keepdim=True)).unsqueeze(0)#\n",
    "\n",
    "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "        dists = dists * self.weight.sign()\n",
    "        \n",
    "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
    "\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    vals = prompt.rsplit(':', 2)\n",
    "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
    "    return vals[0], float(vals[1]), float(vals[2])\n",
    "\n",
    "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
    "    input_normed = F.normalize(input, dim=-1)\n",
    "    target_normed = F.normalize(target, dim=-1)\n",
    "    logits = input_normed @ target_normed.T * logit_scale\n",
    "    if labels is None:\n",
    "        labels = torch.arange(len(input), device=logits.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
    "\n",
    "    def set_cut_pow(self, cut_pow):\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        cutouts_full = []\n",
    "        \n",
    "        min_size_width = min(sideX, sideY)\n",
    "        lower_bound = float(self.cut_size/min_size_width)\n",
    "        \n",
    "        for ii in range(self.cutn):\n",
    "            size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.)) # replace .5 with a result for 224 the default large size is .95\n",
    "          \n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "\n",
    "        cutouts = torch.cat(cutouts, dim=0)\n",
    "\n",
    "        return clamp_with_grad(cutouts, 0, 1)\n",
    "\n",
    "\n",
    "def load_vqgan_model(config_path, checkpoint_path):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
    "        model = vqgan.VQModel(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
    "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
    "        parent_model.eval().requires_grad_(False)\n",
    "        parent_model.init_from_ckpt(checkpoint_path)\n",
    "        model = parent_model.first_stage_model\n",
    "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
    "        model = vqgan.GumbelVQ(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "    else:\n",
    "        raise ValueError(f'unknown model type: {config.model.target}')\n",
    "    del model.loss\n",
    "    return model\n",
    "\n",
    "def resize_image(image, out_size):\n",
    "    ratio = image.size[0] / image.size[1]\n",
    "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
    "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
    "    return image.resize(size, Image.LANCZOS)\n",
    "\n",
    "class GaussianBlur2d(nn.Module):\n",
    "    def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.value = value\n",
    "        if not window:\n",
    "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
    "        if sigma:\n",
    "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
    "            kernel /= kernel.sum()\n",
    "        else:\n",
    "            kernel = torch.ones([1])\n",
    "        self.register_buffer('kernel', kernel)\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.shape\n",
    "        input = input.view([n * c, 1, h, w])\n",
    "        start_pad = (self.kernel.shape[0] - 1) // 2\n",
    "        end_pad = self.kernel.shape[0] // 2\n",
    "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
    "        input = F.conv2d(input, self.kernel[None, None, None, :])\n",
    "        input = F.conv2d(input, self.kernel[None, None, :, None])\n",
    "        return input.view([n, c, h, w])\n",
    "\n",
    "class EMATensor(nn.Module):\n",
    "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
    "    def __init__(self, tensor, decay):\n",
    "        super().__init__()\n",
    "        self.tensor = nn.Parameter(tensor)\n",
    "        self.register_buffer('biased', torch.zeros_like(tensor))\n",
    "        self.register_buffer('average', torch.zeros_like(tensor))\n",
    "        self.decay = decay\n",
    "        self.register_buffer('accum', torch.tensor(1.))\n",
    "        self.update()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update(self):\n",
    "        if not self.training:\n",
    "            raise RuntimeError('update() should only be called during training')\n",
    "\n",
    "        self.accum *= self.decay\n",
    "        self.biased.mul_(self.decay)\n",
    "        self.biased.add_((1 - self.decay) * self.tensor)\n",
    "        self.average.copy_(self.biased)\n",
    "        self.average.div_(1 - self.accum)\n",
    "\n",
    "    def forward(self):\n",
    "        if self.training:\n",
    "            return self.tensor\n",
    "        return self.average\n",
    "  \n",
    "import io\n",
    "import base64\n",
    "def image_to_data_url(img, ext):  \n",
    "    img_byte_arr = io.BytesIO()\n",
    "    img.save(img_byte_arr, format=ext)\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    # ext = filename.split('.')[-1]\n",
    "    prefix = f'data:image/{ext};base64,'\n",
    "    return prefix + base64.b64encode(img_byte_arr).decode('utf-8')\n",
    " \n",
    "\n",
    "def update_random( seed, purpose ):\n",
    "  if seed == -1:\n",
    "    seed = random.seed()\n",
    "    seed = random.randrange(1,99999)\n",
    "    \n",
    "  print( f'Using seed {seed} for {purpose}')\n",
    "  random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  return seed\n",
    "\n",
    "def lerp(a,b,alpha):\n",
    "    return (b-a)*alpha + a\n",
    "    \n",
    "def clear_memory():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MakeCuts Class with Debug Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MakeCuts class with debug functionality\n",
    "cut_sizes_should_use_large_dimension = False #@param {type:'boolean'}\n",
    "cut_size_mode = \"trunc_norm\" #@param [ \"normal\", \"trunc_norm\", \"cut_pow\", \"cut_map\"]\n",
    "cut_oversample = 0#@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "std_factor = 1.5#@param {type:'number'}\n",
    "cut_map_weight = 0.66#@param {type:'number'}\n",
    "cut_map_size_weights = [0.6, 0.4, 0.2, 0.12] #@param\n",
    "#@markdown ----\n",
    "\n",
    "class MakeCutouts3(nn.Module):\n",
    "    def __init__(self, min_cut, cutn, oversample=0., std_factor=1.5, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cutn = cutn\n",
    "        \n",
    "        self.oversample = oversample\n",
    "        self.min_cut = torch.tensor([min_cut])\n",
    "        self.std_factor = std_factor\n",
    "\n",
    "        self.cut_pow = cut_pow\n",
    "        self.cut_map_weight = cut_map_weight\n",
    "        \n",
    "    def simulate(self,input,runs=100):\n",
    "\n",
    "        history = []\n",
    "        for _ in range(runs):\n",
    "            sizes, offsets, extents, weights = self.pick_cuts(input)\n",
    "            history.append(sizes.squeeze())\n",
    "        history = torch.cat( history, dim=0 )\n",
    "\n",
    "        heat = torch.zeros(1,*input.shape[2:4])\n",
    "        weights = weights / weights.sum()\n",
    "        offsets = offsets.clamp_min(0)\n",
    "        for i in range(self.cutn):\n",
    "          heat[...,offsets[i,0]:extents[i,0],offsets[i,1]:extents[i,1]] += 1 / self.cutn\n",
    "\n",
    "        heat = np.pi * 1.6 - heat * ( 1.3 * np.pi )\n",
    "        heat = kornia.color.hsv_to_rgb( torch.stack( (heat, torch.ones_like(heat), torch.ones_like(heat)*1), dim=1))\n",
    "\n",
    "        return history, heat\n",
    "\n",
    "    def pick_sizes(self,dims,cutn,mode,allow_lb=False):\n",
    "\n",
    "        if cut_sizes_should_use_large_dimension:\n",
    "            max_size = dims.max().unsqueeze(0)\n",
    "            min_size = torch.min( dims.min(), self.min_cut).unsqueeze(0)\n",
    "\n",
    "            rand_size_mean = ( dims.prod() * self.min_cut ) ** ( 1/3. )\n",
    "            rand_size_std = min( rand_size_mean - min_size, max_size - rand_size_mean ) * self.std_factor\n",
    "        else:\n",
    "            max_size = torch.max( dims.min(), self.min_cut )[None]\n",
    "            min_size = torch.min( dims.min(), self.min_cut )[None]\n",
    "\n",
    "            rand_size_mean = ( min_size * max_size ) ** (1/2.) \n",
    "            rand_size_std = min( abs(rand_size_mean - min_size), abs(max_size - rand_size_mean) ) * self.std_factor\n",
    "\n",
    "        # use normal distribution... \n",
    "        if mode == 'normal':\n",
    "            sizes = torch.FloatTensor(cutn,1).normal_( rand_size_mean.squeeze(), rand_size_std.squeeze() )\n",
    "            sizes = sizes.clip( min_size, max_size ).int()\n",
    "\n",
    "        # truncated normal distribution\n",
    "        elif mode == 'trunc_norm':\n",
    "            sizes = torch.nn.init.trunc_normal_(torch.FloatTensor(cutn,1), mean=rand_size_mean, std=rand_size_std, a=min_size, b=max_size ).int()\n",
    "\n",
    "        # use cut pow...\n",
    "        elif mode == 'cut_pow':\n",
    "            sizes = torch.lerp( min_size.float(), max_size.float(), torch.rand(cutn,1) ** self.cut_pow ).int()\n",
    "        \n",
    "        elif mode == 'cut_map':\n",
    "            \n",
    "            sizemap = torch.tensor( [min_size, max_size, dims.max(), dims.prod() ** 0.5 ]).int()\n",
    "            sizemap = sizemap.sort().values\n",
    "            sizes = torch.tensor( list( torch.utils.data.WeightedRandomSampler( \n",
    "                cut_map_size_weights, cutn, replacement=True) ))\n",
    "            sizes = sizemap[sizes[:,None]]\n",
    "        \n",
    "        return sizes\n",
    "\n",
    "    def pick_cuts(self,input):\n",
    "        dims = torch.tensor(input.shape[2:4]).unsqueeze(0)\n",
    "        hdims = (dims/2).int()\n",
    "\n",
    "        num_cut_maps = int( self.cutn * self.cut_map_weight )\n",
    "        num_cuts = self.cutn - num_cut_maps\n",
    "\n",
    "        sizes = []\n",
    "\n",
    "        if num_cuts > 0:\n",
    "            sizes.append(self.pick_sizes( dims, num_cuts, cut_size_mode, cut_sizes_should_use_large_dimension ))\n",
    "        if num_cut_maps > 0:\n",
    "            sizes.append(self.pick_sizes( dims, num_cut_maps, 'cut_map' ))\n",
    "        \n",
    "        sizes = torch.cat( sizes )\n",
    "        hsizes = (sizes/2).int()\n",
    "\n",
    "        over = ( sizes * self.oversample )\n",
    "\n",
    "        center = torch.lerp( ( hsizes - over ).float(), ( dims - 1 - ( hsizes - over ) ).float(), torch.rand(self.cutn,2) ).int()\n",
    "        center = torch.where( hsizes > ( dims - 1 - hsizes ), hdims,  # center image in cut if bigger than image\n",
    "                     center.clip(hsizes, dims - 1 - hsizes))          # or clamp cut box to be within bounds\n",
    "\n",
    "        offsets = ( center - hsizes )\n",
    "        extents = ( offsets + sizes )\n",
    "        \n",
    "        # calculate cut area relative to whole\n",
    "        weights = ( extents - offsets ).clamp_max(dims).prod(dim=1)\n",
    "\n",
    "        return sizes, offsets, extents, weights.to(device)\n",
    "\n",
    "    def make_cuts(self,input,cut_size,offsets,extents):\n",
    "\n",
    "        # pad image to be square        \n",
    "        dims = torch.tensor(input.shape[2:4])\n",
    "        padding = ((dims.max() - dims )/2).int()\n",
    "\n",
    "        mean = input.view(3,-1).mean(dim=1)\n",
    "\n",
    "        fill = TF.pad( torch.zeros_like(input), ( padding[1], padding[0], padding[1], padding[0] ), padding_mode='constant', fill=1)\n",
    "        fill = fill * mean.view(1,3,1,1)\n",
    "\n",
    "        input = TF.pad( input, ( padding[1], padding[0], padding[1], padding[0] ), padding_mode='constant')\n",
    "\n",
    "        offsets += padding\n",
    "        extents += padding\n",
    "\n",
    "        fill_rand_mul = torch.FloatTensor(self.cutn).uniform_(0,2.0)\n",
    "\n",
    "        cutouts = []\n",
    "        for i in range( self.cutn ):\n",
    "            cutout = input[...,offsets[i,0]:extents[i,0],offsets[i,1]:extents[i,1]] \\\n",
    "                + fill[...,offsets[i,0]:extents[i,0],offsets[i,1]:extents[i,1]] * fill_rand_mul[i]\n",
    "            cutout = resample(cutout, (cut_size, cut_size))\n",
    "            cutouts.append(cutout)\n",
    " \n",
    "        cutouts = torch.cat(cutouts, dim=0)\n",
    "        return cutouts\n",
    "\n",
    "    def forward(self,input):\n",
    "        sizes, offsets, extents, weights = self.pick_cuts(input)        \n",
    "        cutouts = self.make_cuts(input,self.min_cut,offsets,extents)\n",
    "        \n",
    "        return cutouts, weights\n",
    "\n",
    "def show_images(images, width=20, height=5, columns = 5):\n",
    "    height = max(height, int(len(images)/columns) * height)\n",
    "    plt.figure(figsize=(width,height))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "should_test = False #@param {type:'boolean'}\n",
    "show_histogram = False #@param {type:'boolean'}\n",
    "show_cuts = False #@param {type:'boolean'}\n",
    "\n",
    "if should_test:\n",
    "    a = torch.rand( 1,3,496, 994 )\n",
    "    #a = TF.to_tensor(Image.open('test.png')).unsqueeze(0)\n",
    "\n",
    "    mk = MakeCutouts3( 224, 64, oversample=cut_oversample, std_factor =std_factor )\n",
    "\n",
    "    seed = random.randrange(1,99999) \n",
    "    torch.random.manual_seed( seed )\n",
    "    print('seed:', seed)\n",
    "\n",
    "    if show_histogram:\n",
    "        sizes,heat = mk.simulate(a)\n",
    "\n",
    "        n,bins,patches = plt.hist(sizes,16,density=False, alpha=0.75)\n",
    "        plt.xlabel('Cut Sizes',size=16)\n",
    "        plt.ylabel('Counts',size=16)\n",
    "        plt.title('Cut_Pow Method',size=18)\n",
    "        plt.show()\n",
    "\n",
    "        display.display(TF.to_pil_image(heat[0].cpu()))\n",
    "\n",
    "    if show_cuts:\n",
    "        cuts, weights = mk(a)\n",
    "\n",
    "        images = [ TF.to_pil_image(c[0].cpu()) for c in cuts[:64].split(1)]\n",
    "        show_images(images, columns=5)\n",
    "        \n",
    "          #display.display(TF.to_pil_image(c[0].cpu()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Module Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Loss Module Definitions\n",
    "from typing import cast, Dict, Optional\n",
    "from kornia.augmentation import IntensityAugmentationBase2D\n",
    "\n",
    "class FixPadding(nn.Module):\n",
    "    \n",
    "    def __init__(self, module=None, threshold=1e-12, noise_frac=0.00 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.noise_frac = noise_frac\n",
    "\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        dims = input.shape\n",
    "\n",
    "        if self.module is not None:\n",
    "            input = self.module(input + self.threshold)\n",
    "\n",
    "        light = input.new_empty(dims[0],1,1,1).uniform_(0.,2.)\n",
    "\n",
    "        mixed = input.view(*dims[:2],-1).sum(dim=1,keepdim=True)\n",
    "\n",
    "        black = mixed < self.threshold\n",
    "        black = black.view(-1,1,*dims[2:4]).type(torch.float)\n",
    "        black = kornia.filters.box_blur( black, (5,5) ).clip(0,0.1)/0.1\n",
    "\n",
    "        mean = input.view(*dims[:2],-1).sum(dim=2) / mixed.count_nonzero(dim=2)\n",
    "        mean = ( mean[:,:,None,None] * light ).clip(0,1)\n",
    "\n",
    "        fill = mean.expand(*dims)\n",
    "        if 0 < self.noise_frac:\n",
    "            rng = torch.get_rng_state()\n",
    "            fill = fill + torch.randn_like(mean) * self.noise_frac\n",
    "            torch.set_rng_state(rng)\n",
    "        \n",
    "        if self.module is not None:\n",
    "            input = input - self.threshold\n",
    "\n",
    "        return torch.lerp(input,fill,black)\n",
    "\n",
    "\n",
    "class MyRandomNoise(IntensityAugmentationBase2D):\n",
    "    def __init__(\n",
    "        self,\n",
    "        frac: float = 0.1,\n",
    "        return_transform: bool = False,\n",
    "        same_on_batch: bool = False,\n",
    "        p: float = 0.5,\n",
    "    ) -> None:\n",
    "        super().__init__(p=p, same_on_batch=same_on_batch, p_batch=1.0)\n",
    "        self.frac = frac\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__ + f\"({super().__repr__()})\"\n",
    "\n",
    "    def generate_parameters(self, shape: torch.Size) -> Dict[str, torch.Tensor]:\n",
    "        noise = torch.FloatTensor(1).uniform_(0,self.frac)\n",
    "        \n",
    "        # generate pixel data without throwing off determinism of augs\n",
    "        rng = torch.get_rng_state()\n",
    "        noise = noise * torch.randn(shape)\n",
    "        torch.set_rng_state(rng)\n",
    "\n",
    "        return dict(noise=noise)\n",
    "\n",
    "    def apply_transform(\n",
    "        self, input: torch.Tensor, params: Dict[str, torch.Tensor], transform: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        return input + params['noise'].to(input.device)\n",
    "\n",
    "\n",
    "class MultiClipLoss(nn.Module):\n",
    "    perceptors = {}\n",
    "\n",
    "    def __init__(self, clip_models, normalize_prompt_weights, cutn, cut_weight_pow=0.5, clip_weight=1.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalize_prompt_weights = normalize_prompt_weights\n",
    "\n",
    "        # Load Clip\n",
    "        for cm in { cm['model'] for cm in clip_models }:\n",
    "            p = clip.load(cm, jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "            self.perceptors[cm] = { 'clip':p, 'res': int(p.visual.input_resolution) }\n",
    "\n",
    "        self.perceptor_work = clip_models\n",
    "        for pw in self.perceptor_work:\n",
    "            p = self.perceptors[ pw['model']]\n",
    "            pw['embeds'] = self.get_prompt_embeds( p['clip'], pw['prompt'] )\n",
    "            pw['res'] = p['res']\n",
    "    \n",
    "        self.perceptor_work.sort(key=lambda e: e['res'], reverse=True)\n",
    "        \n",
    "        # Make Cutouts\n",
    "        for p in self.perceptors:\n",
    "            print( p )\n",
    "\n",
    "        self.cut_sizes = list( { p['res'] for p in self.perceptors.values() } )\n",
    "        self.cut_sizes.sort( reverse=True )\n",
    "        \n",
    "        self.cutter = MakeCutouts3(self.cut_sizes[-1], cutn, oversample=0.5, std_factor=1)\n",
    "        self.cut_weight_pow = cut_weight_pow\n",
    "\n",
    "        # Prep Augments\n",
    "        self.noise_fac = 0.1\n",
    "        self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                 std=[0.26862954, 0.26130258, 0.27577711])    \n",
    "            \n",
    "        self.noise_aug = MyRandomNoise(frac=self.noise_fac,p=1.)\n",
    "\n",
    "        self.augs = nn.Sequential(\n",
    "            K.RandomHorizontalFlip(p=0.5),\n",
    "            K.RandomSharpness(0.3,p=0.1),\n",
    "            FixPadding( nn.Sequential(\n",
    "                K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='zeros'), # padding_mode=2\n",
    "                K.RandomPerspective(0.2,p=0.4, ),\n",
    "            )),\n",
    "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
    "            K.RandomGrayscale(p=0.15), \n",
    "            self.noise_aug,\n",
    "        )\n",
    "\n",
    "        self.clip_weight = clip_weight\n",
    "        \n",
    "\n",
    "    def get_prompt_embeds( self, perceptor, text_prompt ):\n",
    "    \n",
    "            texts = [phrase.strip() for phrase in text_prompt.split(\"|\")]\n",
    "            if text_prompt == ['']:\n",
    "                texts = []\n",
    "\n",
    "            prompts_weight_sum = 0\n",
    "            parsed_prompts = []\n",
    "            for prompt in texts:\n",
    "                txt, weight, stop = parse_prompt(prompt)\n",
    "                parsed_prompts.append( [txt,weight,stop] )\n",
    "                prompts_weight_sum += max( weight, 0 )\n",
    "\n",
    "            prompt_embeds = []\n",
    "            for prompt in parsed_prompts:\n",
    "                txt, weight, stop = prompt\n",
    "                clip_token = clip.tokenize(txt).to(device)\n",
    "\n",
    "                if self.normalize_prompt_weights and 0 < prompts_weight_sum:\n",
    "                    weight /= prompts_weight_sum\n",
    "\n",
    "                embed = perceptor.encode_text(clip_token).float()\n",
    "                embed_normed = F.normalize(embed.unsqueeze(0), dim=2)\n",
    "                prompt_embeds.append({'embed_normed':embed_normed,'weight':torch.as_tensor(weight, device=device),'stop':torch.as_tensor(stop, device=device)})\n",
    "        \n",
    "            return prompt_embeds\n",
    "\n",
    "    def make_cuts(self,img,currentres,offsets,extents,i):  \n",
    "           \n",
    "        cuts = self.cutter.make_cuts(img,currentres,offsets,extents)\n",
    "        cuts = clamp_with_grad(cuts,0,1)\n",
    "        cuts = self.augs( cuts )\n",
    "        cuts = self.normalize(cuts)\n",
    "        \n",
    "        return cuts\n",
    "\n",
    "    def forward( self, i, img ):\n",
    "        \n",
    "        sizes, offsets, extents, weights = self.cutter.pick_cuts(img)\n",
    "\n",
    "        weights = weights[:,None] ** self.cut_weight_pow\n",
    "        weights_sum = weights.sum()\n",
    "        \n",
    "        loss = []\n",
    "        \n",
    "        cuts = None\n",
    "        currentres = 0\n",
    "        \n",
    "        rng = torch.get_rng_state()\n",
    "\n",
    "        for pw in self.perceptor_work:\n",
    "            if currentres != pw['res']:\n",
    "                currentres = pw['res']\n",
    "\n",
    "                torch.set_rng_state(rng) \n",
    "                cuts = self.make_cuts( img, currentres, offsets, extents, i )\n",
    "\n",
    "            p = self.perceptors[pw['model']]\n",
    "            iii = p['clip'].encode_image(cuts).float()\n",
    "            input_normed = F.normalize(iii.unsqueeze(1), dim=2)\n",
    "            for prompt in pw['embeds']:\n",
    "                dists = input_normed.sub(prompt['embed_normed']).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "                dists = dists * prompt['weight'].sign()\n",
    "\n",
    "                dists = replace_grad(dists, torch.maximum(dists, prompt['stop']))\n",
    "\n",
    "###############################################################################                \n",
    "                \n",
    "                #l = dists.mean()\n",
    "                l = ( dists * weights ).sum() / weights_sum\n",
    "                \n",
    "###############################################################################    \n",
    "                \n",
    "                loss.append(l * prompt['weight'].abs() * pw['weight'])\n",
    "\n",
    "        return loss\n",
    "\n",
    "class MSEDecayLoss(nn.Module):\n",
    "    def __init__(self, init_weight, mse_decay_rate, mse_epoches, mse_quantize ):\n",
    "        super().__init__()\n",
    "      \n",
    "        self.init_weight = init_weight\n",
    "        self.has_init_image = False\n",
    "        self.mse_decay = init_weight / mse_epoches if init_weight else 0 \n",
    "        self.mse_decay_rate = mse_decay_rate\n",
    "        self.mse_weight = init_weight\n",
    "        self.mse_epoches = mse_epoches\n",
    "        self.mse_quantize = mse_quantize\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def set_target( self, z_tensor, model ):\n",
    "        z_tensor = z_tensor.detach().clone()\n",
    "        if self.mse_quantize:\n",
    "            z_tensor = vector_quantize(z_tensor.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)#z.average\n",
    "        self.z_orig = z_tensor\n",
    "          \n",
    "    def forward( self, i, z ):\n",
    "        if self.is_active(i):\n",
    "            return F.mse_loss(z, self.z_orig) * self.mse_weight / 2\n",
    "        return 0\n",
    "        \n",
    "    def is_active(self, i):\n",
    "        if not self.init_weight:\n",
    "          return False\n",
    "        if i <= self.mse_decay_rate and not self.has_init_image:\n",
    "          return False\n",
    "        return True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step( self, i ):\n",
    "\n",
    "        if i % self.mse_decay_rate == 0 and i != 0 and i < self.mse_decay_rate * self.mse_epoches:\n",
    "            \n",
    "            if self.mse_weight - self.mse_decay > 0 and self.mse_weight - self.mse_decay >= self.mse_decay:\n",
    "              self.mse_weight -= self.mse_decay\n",
    "            else:\n",
    "              self.mse_weight = 0\n",
    "            print(f\"updated mse weight: {self.mse_weight}\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "  \n",
    "class TVLoss(nn.Module):\n",
    "    def forward(self, input):\n",
    "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "        diff = x_diff**2 + y_diff**2 + 1e-8\n",
    "        return diff.mean(dim=1).sqrt().mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "id": "agEFaX64bjdj"
   },
   "outputs": [],
   "source": [
    "#@title Random Inits\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def rand_perlin_2d(shape, res, fade = lambda t: 6*t**5 - 15*t**4 + 10*t**3):\n",
    "    delta = (res[0] / shape[0], res[1] / shape[1])\n",
    "    d = (shape[0] // res[0], shape[1] // res[1])\n",
    "    \n",
    "    grid = torch.stack(torch.meshgrid(torch.arange(0, res[0], delta[0]), torch.arange(0, res[1], delta[1])), dim = -1) % 1\n",
    "    angles = 2*math.pi*torch.rand(res[0]+1, res[1]+1)\n",
    "    gradients = torch.stack((torch.cos(angles), torch.sin(angles)), dim = -1)\n",
    "    \n",
    "    tile_grads = lambda slice1, slice2: gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]].repeat_interleave(d[0], 0).repeat_interleave(d[1], 1)\n",
    "    dot = lambda grad, shift: (torch.stack((grid[:shape[0],:shape[1],0] + shift[0], grid[:shape[0],:shape[1], 1] + shift[1]  ), dim = -1) * grad[:shape[0], :shape[1]]).sum(dim = -1)\n",
    "    \n",
    "    n00 = dot(tile_grads([0, -1], [0, -1]), [0,  0])\n",
    "    n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0])\n",
    "    n01 = dot(tile_grads([0, -1],[1, None]), [0, -1])\n",
    "    n11 = dot(tile_grads([1, None], [1, None]), [-1,-1])\n",
    "    t = fade(grid[:shape[0], :shape[1]])\n",
    "    return math.sqrt(2) * torch.lerp(torch.lerp(n00, n10, t[..., 0]), torch.lerp(n01, n11, t[..., 0]), t[..., 1])\n",
    "\n",
    "def rand_perlin_2d_octaves( desired_shape, octaves=1, persistence=0.5):\n",
    "    shape = torch.tensor(desired_shape)\n",
    "    shape = 2 ** torch.ceil( torch.log2( shape ) )\n",
    "    shape = shape.type(torch.int)\n",
    "\n",
    "    max_octaves = int(min(octaves,math.log(shape[0])/math.log(2), math.log(shape[1])/math.log(2)))\n",
    "    res = torch.floor( shape / 2 ** max_octaves).type(torch.int)\n",
    "\n",
    "    noise = torch.zeros(list(shape))\n",
    "    frequency = 1\n",
    "    amplitude = 1\n",
    "    for _ in range(max_octaves):\n",
    "        noise += amplitude * rand_perlin_2d(shape, (frequency*res[0], frequency*res[1]))\n",
    "        frequency *= 2\n",
    "        amplitude *= persistence\n",
    "    \n",
    "    return noise[:desired_shape[0],:desired_shape[1]]\n",
    "\n",
    "def rand_perlin_rgb( desired_shape, amp=0.1, octaves=6 ):\n",
    "  r = rand_perlin_2d_octaves( desired_shape, octaves )\n",
    "  g = rand_perlin_2d_octaves( desired_shape, octaves )\n",
    "  b = rand_perlin_2d_octaves( desired_shape, octaves )\n",
    "  rgb = ( torch.stack((r,g,b)) * amp + 1 ) * 0.5\n",
    "  return rgb.unsqueeze(0).clip(0,1).to(device)\n",
    "\n",
    "\n",
    "def pyramid_noise_gen(shape, octaves=5, decay=1.):\n",
    "    n, c, h, w = shape\n",
    "    noise = torch.zeros([n, c, 1, 1])\n",
    "    max_octaves = int(min(math.log(h)/math.log(2), math.log(w)/math.log(2)))\n",
    "    if octaves is not None and 0 < octaves:\n",
    "      max_octaves = min(octaves,max_octaves)\n",
    "    for i in reversed(range(max_octaves)):\n",
    "        h_cur, w_cur = h // 2**i, w // 2**i\n",
    "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
    "        noise += ( torch.randn([n, c, h_cur, w_cur]) / max_octaves ) * decay**( max_octaves - (i+1) )\n",
    "    return noise\n",
    "\n",
    "def rand_z(model, toksX, toksY):\n",
    "    e_dim = model.quantize.e_dim\n",
    "    n_toks = model.quantize.n_e\n",
    "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
    "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
    "\n",
    "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
    "    z = one_hot @ model.quantize.embedding.weight\n",
    "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def make_rand_init( mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f ):\n",
    "\n",
    "  if mode == 'VQGAN ZRand':\n",
    "    return rand_z(model, toksX, toksY)\n",
    "  elif mode == 'Perlin Noise':\n",
    "    rand_init = rand_perlin_rgb((toksY * f, toksX * f), perlin_weight, perlin_octaves )\n",
    "    z, *_ = model.encode(rand_init * 2 - 1)\n",
    "    return z\n",
    "  elif mode == 'Pyramid Noise':\n",
    "    rand_init = pyramid_noise_gen( (1,3,toksY * f, toksX * f), pyramid_octaves, pyramid_decay).to(device)\n",
    "    rand_init = ( rand_init * 0.5 + 0.5 ).clip(0,1)\n",
    "    z, *_ = model.encode(rand_init * 2 - 1)\n",
    "    return z\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining Setup to Create Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "id": "Nxcx0j6zmePl"
   },
   "outputs": [],
   "source": [
    "# Setup Save Locations\n",
    "\n",
    "import json\n",
    "from PIL import Image, ExifTags\n",
    "from PIL.PngImagePlugin import PngImageFile, PngInfo\n",
    "\n",
    "#@title  Set VQGAN Model Save Location\n",
    "#@markdown It's a lot faster to load model files from google drive than to download them every time you want to use this notebook.\n",
    "save_vqgan_models_to_drive = False #@param {type: 'boolean'}\n",
    "download_all = False \n",
    "test_all = False\n",
    "vqgan_path_on_google_drive = \"/content/drive/MyDrive/Art/Models/VQGAN/\" #@param {type: 'string'}\n",
    "vqgan_path_on_google_drive += \"/\" if not vqgan_path_on_google_drive.endswith('/') else \"\"\n",
    "\n",
    "#@markdown ###  Save Runs to Google Drive\n",
    "#@markdown Should all the images during the run be saved to google drive?\n",
    "save_output_to_drive = False #@param {type:'boolean'}\n",
    "output_path_on_google_drive = \"/content/drive/MyDrive/Art/\" #@param {type: 'string'}\n",
    "output_path_on_google_drive += \"/\" if not output_path_on_google_drive.endswith('/') else \"\"\n",
    "\n",
    "#@markdown Should the run params be saved as well for future reference? \n",
    "save_params_to_json = True #@param {type:'boolean'}\n",
    "\n",
    "#@markdown When saving the images, how much should be included in the name?\n",
    "shortname_limit = 50 #@param {type: 'number'}\n",
    "filename_limit = 250\n",
    "\n",
    "if save_vqgan_models_to_drive or save_output_to_drive:\n",
    "    from google.colab import drive    \n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "vqgan_model_path = \"./models/\"\n",
    "if save_vqgan_models_to_drive:\n",
    "    vqgan_model_path = vqgan_path_on_google_drive\n",
    "    !mkdir -p \"$vqgan_path_on_google_drive\"\n",
    "\n",
    "save_output_path = \"./content/\"\n",
    "if save_output_to_drive:\n",
    "    save_output_path = output_path_on_google_drive\n",
    "!mkdir -p \"$save_output_path\"\n",
    "\n",
    "model_download={\n",
    "  \"vqgan_imagenet_f16_1024\":\n",
    "      [[\"vqgan_imagenet_f16_1024.yaml\", \"https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1\"],\n",
    "      [\"vqgan_imagenet_f16_1024.ckpt\", \"http://batbot.tv/ai/models/VQGAN/imagenet_1024_slim.ckpt\"]],\n",
    "  \"vqgan_imagenet_f16_16384\": \n",
    "      [[\"vqgan_imagenet_f16_16384.yaml\", \"http://batbot.tv/ai/models/VQGAN/imagenet_16384.yaml\"],\n",
    "      [\"vqgan_imagenet_f16_16384.ckpt\", \"http://batbot.tv/ai/models/VQGAN/imagenet_16384_slim.ckpt\"]],\n",
    "  \"vqgan_openimages_f8_8192\":\n",
    "      [[\"vqgan_openimages_f8_8192.yaml\", \"https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1\"],\n",
    "      [\"vqgan_openimages_f8_8192.ckpt\", \"http://batbot.tv/ai/models/VQGAN/gumbel_f8_8192.ckpt\"]],\n",
    "  \"coco\":\n",
    "      [[\"coco_first_stage.yaml\", \"http://batbot.tv/ai/models/VQGAN/coco_first_stage.yaml\"],\n",
    "      [\"coco_first_stage.ckpt\", \"http://batbot.tv/ai/models/VQGAN/coco_first_stage.ckpt\"]],\n",
    "  \"faceshq\":\n",
    "      [[\"faceshq.yaml\", \"http://batbot.tv/ai/models/VQGAN/faceshq.yaml\"],\n",
    "      [\"faceshq.ckpt\", \"http://batbot.tv/ai/models/VQGAN/faceshq2_slim.ckpt\"]],\n",
    "  \"wikiart_1024\":\n",
    "      [[\"wikiart_1024.yaml\", \"http://batbot.tv/ai/models/VQGAN/WikiArt_augmented_Steps_7mil_finetuned_1mil.yaml\"],\n",
    "      [\"wikiart_1024.ckpt\", \"http://batbot.tv/ai/models/VQGAN/WikiArt_augmented_Steps_7mil_finetuned_1mil.ckpt\"]],\n",
    "  \"wikiart_16384\":\n",
    "      [[\"wikiart_16384.yaml\", \"http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml\"],\n",
    "      [\"wikiart_16384.ckpt\", \"http://batbot.tv/ai/models/VQGAN/wikiart_16384_slim.ckpt\"]],\n",
    "  \"sflckr\":\n",
    "      [[\"sflckr.yaml\", \"https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1\"],\n",
    "      [\"sflckr.ckpt\", \"http://batbot.tv/ai/models/VQGAN/sflckr_slim.ckpt\"]],\n",
    "  }\n",
    "\n",
    "loaded_model = None\n",
    "loaded_model_name = None\n",
    "def dl_vqgan_model(image_model):\n",
    "    for curl_opt in model_download[image_model]:\n",
    "        modelpath = f'{vqgan_model_path}{curl_opt[0]}'\n",
    "        if not path.exists(modelpath):\n",
    "            print(f'downloading {modelpath} from {curl_opt[1]}')\n",
    "            !curl -L -o {modelpath} '{curl_opt[1]}'\n",
    "        else:\n",
    "            print(f'found existing {curl_opt[0]}')\n",
    "\n",
    "def get_vqgan_model(image_model):\n",
    "    global loaded_model\n",
    "    global loaded_model_name\n",
    "    if loaded_model is None or loaded_model_name != image_model:\n",
    "        dl_vqgan_model(image_model)\n",
    "    \n",
    "        print(f'loading {image_model} vqgan checkpoint')\n",
    "\n",
    "        \n",
    "        vqgan_config= vqgan_model_path + model_download[image_model][0][0]\n",
    "        vqgan_checkpoint= vqgan_model_path + model_download[image_model][1][0]\n",
    "        print('vqgan_config',vqgan_config)\n",
    "        print('vqgan_checkpoint',vqgan_checkpoint)\n",
    "\n",
    "        model = load_vqgan_model(vqgan_config, vqgan_checkpoint).to(device)\n",
    "        if image_model == 'vqgan_openimages_f8_8192':\n",
    "            model.quantize.e_dim = 256\n",
    "            model.quantize.n_e = model.quantize.n_embed\n",
    "            model.quantize.embedding = model.quantize.embed\n",
    "\n",
    "        loaded_model = model\n",
    "        loaded_model_name = image_model\n",
    "\n",
    "    return loaded_model\n",
    "\n",
    "emoji_match = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def slugify(value):\n",
    "    value = str(value)\n",
    "    value = re.sub(r':([-\\d.]+)', ' [\\\\1]', value)\n",
    "    value = re.sub(r'[|]','; ',value)\n",
    "    value = re.sub(r'[<>:\"/\\\\|?*]', ' ', value)\n",
    "    value = emoji_match.sub( r'', value )\n",
    "    return value.strip()\n",
    "\n",
    "save_output_path_for_run = \"\"\n",
    "def set_drive_output_subfolder( seconds=True ):\n",
    "    now = datetime.now()    \n",
    "    if seconds:\n",
    "        ts = now.strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    else:\n",
    "        ts = now.strftime(\"%Y-%m-%d %H-%M\")\n",
    "\n",
    "    run_args.timestamp = ts\n",
    "\n",
    "    global save_output_path_for_run\n",
    "    save_output_path_for_run = path.join( save_output_path, ts )\n",
    "\n",
    "has_saved_params_json = False\n",
    "def BuildRunRecord():\n",
    "    set_drive_output_subfolder()\n",
    "\n",
    "    global has_saved_params_json\n",
    "    has_saved_params_json = False\n",
    "\n",
    "\n",
    "def OutputRunRecordIfNeeded():\n",
    "    global has_saved_params_json\n",
    "    if save_params_to_json and not has_saved_params_json:\n",
    "        has_saved_params_json = True\n",
    "\n",
    "        fname = f'run_{run_args.gen_seed}.json'\n",
    "        fpath = path.join(save_output_path_for_run,fname)\n",
    "        with open(fpath, \"w\") as outfile:\n",
    "            json.dump( vars(run_args), outfile, indent=2)\n",
    "\n",
    "def get_filename(text, seed, i, ext):\n",
    "    text = re.split(r'[|:;]',text, 1)[0][:shortname_limit]\n",
    "    text = slugify(text)\n",
    "\n",
    "    ts =  run_args.timestamp\n",
    "    return f'{ts} r{seed}; {text}{ext}'\n",
    "\n",
    "last_image_saved = None\n",
    "def SaveRunIteration( i, pil ):\n",
    "    if not save_art_output:\n",
    "        return        \n",
    "        \n",
    "    if not path.exists(save_output_path_for_run):\n",
    "        os.makedirs(save_output_path_for_run)\n",
    "    \n",
    "    OutputRunRecordIfNeeded()\n",
    "\n",
    "    def makename(text, ts,  i):\n",
    "        text = re.split(r'[|:;]',text, 1)[0][:shortname_limit]\n",
    "        text = slugify(text)\n",
    "        return f'{ts} i{i}; {text}.png'\n",
    "\n",
    "    run_args.i = i\n",
    "\n",
    "    name = makename( run_args.text_prompt, run_args.timestamp, i )\n",
    "\n",
    "    global last_image_saved\n",
    "    last_image_saved = path.join( save_output_path_for_run, name ) \n",
    "\n",
    "    if save_params_to_exif:\n",
    "        jsondata = json.dumps( vars(run_args) ) \n",
    "\n",
    "        info = PngInfo()\n",
    "        info.add_text(\"RunArgs\", jsondata)\n",
    "\n",
    "        pil.save(last_image_saved, pnginfo=info)\n",
    "\n",
    "    else:\n",
    "        pil.save(last_image_saved)\n",
    "\n",
    "for model in model_download.keys():\n",
    "    if test_all:\n",
    "        try:\n",
    "            vqtest = get_vqgan_model(model)\n",
    "            del vqtest\n",
    "        except:\n",
    "            print(\"Failed to load \"+model)\n",
    "    elif save_vqgan_models_to_drive and download_all:\n",
    "        dl_vqgan_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AUMYhUaEbnkL"
   },
   "outputs": [],
   "source": [
    "#@title  Set Display Rate\n",
    "#@markdown If `use_automatic_display_schedule` is enabled, the image will be output frequently at first, and then more spread out as time goes on. Turn this off if you want to specify the display rate yourself.\n",
    "use_automatic_display_schedule = False #@param {type:'boolean'}\n",
    "should_display_init = True #@param {type:'boolean'}\n",
    "display_every = 5 #@param {type:'number'}\n",
    "\n",
    "def should_checkin(i):\n",
    "  if i == 0:\n",
    "    return should_display_init\n",
    "\n",
    "  if i == max_iter: \n",
    "    return True \n",
    "\n",
    "  if not use_automatic_display_schedule:\n",
    "    return i % display_every == 0\n",
    "\n",
    "  schedule = [[100,25],[500,50],[1000,100],[2000,200]]\n",
    "  for s in schedule:\n",
    "    if i <= s[0]:\n",
    "      return i % s[1] == 0\n",
    "  return i % 500 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbrDcvJNNLO0"
   },
   "source": [
    "## Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title  Madlib Prompts\n",
    "#@markdown When enabled, prompts will be generated at random using a templating language. This allows for very expressive prompts and combines well with batch runs but does require doing some code edits.\n",
    "#@markdown - Check out my [demo notebook](https://colab.research.google.com/drive/1gGwD0zDvyx0OnJU4KLjE8HrwnTlzI2rS?usp=sharing) for more explanation and examples\n",
    "AllowSimpleMadlibSyntax = True #on by default, to support {|} format in prompts\n",
    "UseFullMadlibTemplates = False #@param {type:'boolean'}\n",
    "\n",
    "## Template definition setup\n",
    "\n",
    "ml_templates = [ \n",
    "  { \n",
    "      \"prompt\" : \"A {something}, {something} garden by {James Gurney|Greg Rutkowski}\", \n",
    "      \"lookup\" : {}, \n",
    "      \"weight\":2 # increase the frequency of this prompt\n",
    "  },\n",
    "  {   \n",
    "      \"prompt\" : \"A beautiful {color} {sunrise|sunset} over the {something} {something} garden by {James Gurney|Greg Rutkowski}\", \n",
    "      \"lookup\" : {\n",
    "          \"something\": [\"enchanted\"], # overriding the base set\n",
    "          \"color\" : ['blue','red','purple','pink'] # adding extra data for only this prompt\n",
    "      } \n",
    "  } \n",
    "] \n",
    "ml_lookup = {\n",
    "    \"something\":['vivid','enchanted','forbidden','rotten']\n",
    "}\n",
    "ml_options = {\n",
    "    \"avoid_repeats\" : True, \n",
    "    \"fix_indefinite_articles\" : True \n",
    "} \n",
    "\n",
    "## -------------------------------- \n",
    "## supporting code from linked notebook\n",
    "\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def madlib_prompt(base, lookup = {},\n",
    "        options = { \n",
    "            \"avoid_repeats\" : True, \n",
    "            \"fix_indefinite_articles\" : True \n",
    "        }):\n",
    "    lookup_cardstack = {}\n",
    "\n",
    "    def on_madlib_sub( match ):\n",
    "        opt = match.group(2).split('|')\n",
    "\n",
    "        key = opt[0]\n",
    "\n",
    "        # do a lookup if there's only one option in the brackets\n",
    "        if len(opt) == 1 and lookup.get(key):\n",
    "\n",
    "            # discard used words to avoid repeats, unless no words remain\n",
    "            if options[\"avoid_repeats\"]:\n",
    "\n",
    "                if len(lookup_cardstack.get(key) or []) == 0:\n",
    "                    lookup_cardstack[key] = copy.copy( lookup[key] )\n",
    "\n",
    "                g2 = random.choice( lookup_cardstack[key] )\n",
    "                lookup_cardstack[key].remove(g2)\n",
    "\n",
    "               \n",
    "            else:\n",
    "                g2 = random.choice( lookup.get(key) )\n",
    "\n",
    "        # choose one of the options to fill this space\n",
    "        else:\n",
    "            g2 = random.choice( opt )\n",
    "\n",
    "        # if the previous word is 'A' or 'An', figure out if the 'n' is needed\n",
    "        g1 = match.group(1)\n",
    "        if g1 is not None:\n",
    "            if options[\"fix_indefinite_articles\"]:\n",
    "                if g2[0].lower() in list(\"aeiou\"):\n",
    "                    g1 = g1[0] + 'n'\n",
    "                else:\n",
    "                    g1 = g1[0]\n",
    "            g2 = g1 + ' ' + g2\n",
    "        \n",
    "        return g2\n",
    "    \n",
    "    # find madlib spots, and replace\n",
    "    return re.sub(r\"(\\b[Aa]n? )?{([^{}]*)}\", on_madlib_sub, base )\n",
    "\n",
    "def madlib_template( templates, global_lookup = {}, \n",
    "        options = { \n",
    "            \"avoid_repeats\" : True, \n",
    "            \"fix_indefinite_articles\" : True \n",
    "        }):\n",
    "\n",
    "    # get prompt template weights and pick one\n",
    "    weights = [ p.get('weight') or 1 for p in templates]\n",
    "    t = random.choices( templates, weights=weights, k=1 )[0]\n",
    "    \n",
    "    lookup = { **global_lookup, **(t.get('lookup') or {}) }\n",
    "    lookup_cardstack = {}\n",
    "\n",
    "    def on_madlib_sub( match ):\n",
    "        opt = match.group(2).split('|')\n",
    "\n",
    "        key = opt[0]\n",
    "\n",
    "        # do a lookup if there's only one option in the brackets\n",
    "        if len(opt) == 1 and lookup.get(key):\n",
    "\n",
    "            # discard used words to avoid repeats, unless no words remain\n",
    "            if options[\"avoid_repeats\"]:\n",
    "\n",
    "                if len(lookup_cardstack.get(key) or []) == 0:\n",
    "                    lookup_cardstack[key] = copy.copy( lookup[key] )\n",
    "\n",
    "                g2 = random.choice( lookup_cardstack[key] )\n",
    "                lookup_cardstack[key].remove(g2)\n",
    "\n",
    "            else:\n",
    "                g2 = random.choice( lookup.get(key) )\n",
    "          \n",
    "        # or just pick one of the given options  \n",
    "        else:\n",
    "            g2 = random.choice( opt )\n",
    "\n",
    "        g1 = match.group(1)\n",
    "        if g1 is not None:\n",
    "\n",
    "            # if the previous word is 'A' or 'An', figure out if the 'n' is needed\n",
    "            if options[\"fix_indefinite_articles\"]:\n",
    "                    if g2[0].lower() in list(\"aeiou\"):\n",
    "                        g1 = g1[0] + 'n'\n",
    "                    else:\n",
    "                        g1 = g1[0]\n",
    "            g2 = g1 + ' ' + g2\n",
    "            \n",
    "        return g2\n",
    "    \n",
    "    # find madlib spots, and replace\n",
    "    return re.sub(r\"(\\b[Aa]n? )?{([^{}]*)}\", on_madlib_sub, t['prompt'] )\n",
    "\n",
    "## -------------------------------- \n",
    "## hook into the generation\n",
    "def get_prompt():\n",
    "\n",
    "    if UseFullMadlibTemplates:\n",
    "        # generate prompt from template        \n",
    "        if 'base_prompt' in run_args:\n",
    "            del run_args.base_prompt\n",
    "        return madlib_template( ml_templates, ml_lookup, ml_options )\n",
    "        \n",
    "    # should still use simple madlibbing functionality \n",
    "    # when not using an advanced template\n",
    "    if AllowSimpleMadlibSyntax:\n",
    "        return madlib_prompt(run_args.base_prompt)\n",
    "\n",
    "    return run_args.base_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "form",
    "id": "AXDXArQrNR9E"
   },
   "outputs": [],
   "source": [
    "#@title  Batch Runs\n",
    "#@markdown When enabled, this notebook will continue to make art. Use at your own risk! This can cause your colab account to be locked out temporarily if you abuse it too much.\n",
    "#@markdown - When doing batch runs, the gen seed will be randomized for each piece.\n",
    "#@markdown - If `batch_run_limit` is -1, it will keep going until you run out of disk space or Google disconnects you!\n",
    "\n",
    "BatchRunOverride = False #@param {type:'boolean'}\n",
    "batch_run_limit = -1 #@param {type:'number'}\n",
    "headless = False #@param {type:'boolean'}\n",
    "\n",
    "#@markdown Note: If you edit the code in this cell you can add some parameter randomization here as well.\n",
    "\n",
    "def rand_float( start, stop ):\n",
    "    return torch.FloatTensor(1).uniform_(start,stop).item()\n",
    "\n",
    "def randomize_run():        \n",
    "    global run_args\n",
    "\n",
    "    # Setup Random Seed\n",
    "    if BatchRunOverride:\n",
    "        run_args.gen_seed = update_random( -1, 'randomized run') \n",
    "    else:\n",
    "        run_args.gen_seed = update_random( run_args.gen_seed, 'randomized run' )\n",
    "   \n",
    "    prompt = get_prompt()\n",
    "    prompt = prompt.encode('utf-16', 'surrogatepass').decode('utf-16') #fix emoji issue...\n",
    "\n",
    "    run_args.text_prompt = prompt\n",
    "\n",
    "    # run_args.clip1_weight = random.choice( [ 0.33, 0.5, 0.66 ])\n",
    "    # run_args.tv_weight = 0\n",
    "\n",
    "    # run_args.rand_init_mode = random.choice( [ \"Perlin Noise\", \"Pyramid Noise\"] )\n",
    "\n",
    "    # run_args.perlin_octaves = random.choice( [random.randint(2,3), random.randint(6,8)] )\n",
    "    # run_args.perlin_weight = rand_float(0.1,0.4)\n",
    "    \n",
    "    # run_args.pyramid_octaves = random.randint(7,8)\n",
    "    # run_args.pyramid_decay = rand_float(0.7,0.9)\n",
    "\n",
    "    # run_args.mse_weight = rand_float( 0.1,0.2 )\n",
    "\n",
    "    # run_args.learning_rate = random.choice( [0.4,0.5,0.2] )\n",
    "    # run_args.learning_rate_epoch = random.choice( [0.4,0.2,0.3] )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "id": "LCJIEiJpNTmj"
   },
   "outputs": [],
   "source": [
    "#@title ##  Noise Burst\n",
    "#@markdown Increase the amount of noise before sending the image to CLIP. This creates a large burst of noise with an exponential falloff every so often (based on the parameters below).\n",
    "\n",
    "DoNoiseBurst = False #@param {type:'boolean'}\n",
    "noise_frac_base = 0.1 #@param {type:'number'}\n",
    "#@markdown  ---\n",
    "noise_frac_burst = 0.3 #@param {type:'number'}\n",
    "noise_burst_start = 150 #@param {type:'number'}\n",
    "noise_burst_end = 350 #@param {type:'number'}\n",
    "noise_burst_freq = 100 #@param {type:'number'}\n",
    "noise_burst_length = 100 #@param {type:'number'}\n",
    "\n",
    "def get_noise_frac(i):\n",
    "    if not DoNoiseBurst:\n",
    "        return run_args.noise_frac_base\n",
    "        \n",
    "    nb_start = run_args.noise_burst_start\n",
    "    nb_end = run_args.noise_burst_end\n",
    "\n",
    "    noise = run_args.noise_frac_base\n",
    "\n",
    "    if nb_start <= i and i <= nb_end:\n",
    "        nb_freq = run_args.noise_burst_freq\n",
    "        nb_length = run_args.noise_burst_length\n",
    "        alpha = math.exp( -10 * ( (i + nb_start) % nb_freq ) / nb_length )\n",
    "        noise +=  run_args.noise_frac_burst * alpha\n",
    "\n",
    "    return noise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellView": "form",
    "id": "zX9CYaFONVD2"
   },
   "outputs": [],
   "source": [
    "#@title #  Cut Distribution Tuning\n",
    "\n",
    "#@markdown Cut sizes are picked by one of two methods:\n",
    "#@markdown 1. A truncated normal about an average point between the perceptor size and the smaller dimension of the image\n",
    "#@markdown 2. A weighted choice from a map of 4 possible sizes:<br>  ```[Perceptor size, Min image dimension, Max image dimension, Geometric average of min and max dimension]```\n",
    "#@markdown\n",
    "#@markdown `cut_map_weight` determines how many of the cuts are allocated via truncated normal vs weighted map. The default tuning is set to match the distribution of jbusted1's setup, with a small portion of cuts also being allocated for medium-large and global to help with overall image cohesion.\n",
    "\n",
    "cut_map_weight = 0.66#@param {type:'number'}\n",
    "cut_map_size_weights = [0.6, 0.4, 0.2, 0.12] #@param\n",
    "\n",
    "std_factor = 1.5#@param {type:'number'}\n",
    "#@markdown - Used in truncated normal to pick random cut sizes\n",
    "\n",
    "cut_oversample = 0#@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "#@markdown - Increase to add oversampling of the edges to get more even cut distribution\n",
    "\n",
    "#@markdown ### Simulate\n",
    "#@markdown You can use the options below to render out a heatmap and histogram to get a better idea of how the MakeCuts method is picking cut sizes.\n",
    "cut_test_show_histogram = False #@param {type:'boolean'}\n",
    "cut_test_show_cuts = False\n",
    "cut_test_count = 64 #@param {type:'number'}\n",
    "\n",
    "try: cut_test_width = width\n",
    "except NameError: cut_test_width = 994\n",
    "\n",
    "try: cut_test_height = height\n",
    "except NameError: cut_test_height = 496\n",
    "\n",
    "cut_test_perceptor_dim = 224 #@param {type:'number'}\n",
    "\n",
    "def test_makecuts():\n",
    "    if cut_test_show_histogram or cut_test_show_cuts:\n",
    "        \n",
    "        mk = MakeCutouts3( cut_test_perceptor_dim, cut_test_count, oversample=cut_oversample, std_factor =std_factor )\n",
    "    \n",
    "        if cut_test_show_histogram:\n",
    "            \n",
    "            a = torch.rand( 1,3,cut_test_height, cut_test_width )\n",
    "            #a = TF.to_tensor(Image.open('test.png')).unsqueeze(0)\n",
    "\n",
    "            seed = random.randrange(1,99999) \n",
    "            torch.random.manual_seed( seed )\n",
    "            print('seed:', seed)\n",
    "\n",
    "            sizes,heat = mk.simulate(a)\n",
    "\n",
    "            n,bins,patches = plt.hist(sizes,16,density=False, alpha=0.75)\n",
    "            plt.xlabel('Cut Sizes',size=16)\n",
    "            plt.ylabel('Counts',size=16)\n",
    "            plt.title('Cut_Pow Method',size=18)\n",
    "            plt.show()\n",
    "\n",
    "            display.display(TF.to_pil_image(heat[0].cpu()))\n",
    "\n",
    "        if cut_test_show_cuts:\n",
    "            cuts, weights = mk(a)\n",
    "\n",
    "            images = [ TF.to_pil_image(c[0].cpu()) for c in cuts[:cut_test_count].split(1)]\n",
    "            show_images(images, columns=5)\n",
    "        \n",
    "test_makecuts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellView": "form",
    "id": "ZceqGjQSNWXh"
   },
   "outputs": [],
   "source": [
    "#@title ### Video\n",
    "#@markdown If you want to generate a video of the run, you need to save the frames as you go. The more frequently you save, the longer the video but the slower it will take to generate.\n",
    "save_frames_for_video = False #@param {type:'boolean'}\n",
    "autogenerate_video_after_run = True #@param {type:'boolean'}\n",
    "\n",
    "video_start =  0#@param {type:'number'}\n",
    "video_step =   3#@param {type:'number'}\n",
    "\n",
    "#@markdown Setting a higher frequency will make a longer video, and a higher framerate will make a shorter video.\n",
    "fps = 24 #@param{type:'number'}\n",
    "\n",
    "def should_save_for_video(i):\n",
    "    if not save_frames_for_video or i < video_start:\n",
    "        return False\n",
    "    \n",
    "    return (i - video_start) % video_step == 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_frame_for_video(i, pil):\n",
    "    frame = (i - video_start) // video_step\n",
    "    pil.save(f'steps/step{frame:04}.png')   \n",
    "@torch.no_grad()\n",
    "def save_video():\n",
    "    if not save_frames_for_video:\n",
    "        print('You must first enable save_frames_for_video in the Advanced section to use this feature.')\n",
    "        return\n",
    "\n",
    "    !mkdir -p \"./content/video/\"\n",
    "    vname = \"./content/video/\"+get_filename(run_args.text_prompt,run_args.gen_seed,None,'.mp4')\n",
    "          \n",
    "    !ffmpeg -y -v 1 -framerate $fps -i steps/step%04d.png -r $fps -vcodec libx264 -crf 32 -pix_fmt yuv420p \"$vname\"\n",
    "    if save_output_to_drive:\n",
    "        if not path.exists(save_output_path_for_run):\n",
    "            os.makedirs(save_output_path_for_run)\n",
    "        OutputRunRecordIfNeeded()\n",
    "        !cp \"$vname\" \"$save_output_path_for_run\"\n",
    "\n",
    "    mp4 = open(vname,'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    display.display( display.HTML(f'<video controls><source src=\"{data_url}\" type=\"video/mp4\"></video>') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "form",
    "id": "hIXJZPzBNXry"
   },
   "outputs": [],
   "source": [
    "#@title ###  Animation\n",
    "#@markdown Add optional zoom and rotate as you generate, to make cool animated videos. `save_frames_for_video` must be enabled above as well for this to really do anything useful.\n",
    "\n",
    "UseAnimationOverride = False #@param {type:'boolean'}\n",
    "\n",
    "#@markdown You should sync these to the video start/step times for best results.\n",
    "anim_start = 100  #@param {type:'number'}\n",
    "anim_step = 3 #@param {type:'number'}\n",
    "\n",
    "#@markdown Animation parameters for basic animation. Modify the code in this cell to create more complex behaviors.\n",
    "zoom_amount = 4  #@param {type:'number'}\n",
    "rotate_amount = -0.5  #@param {type:'number'}\n",
    "\n",
    "def should_do_animation_step(i):\n",
    "    if not UseAnimationOverride or i < anim_start:\n",
    "        return False\n",
    "    return (i - anim_start) % anim_step == 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def do_anim_step():\n",
    "    global z\n",
    "    zoom = zoom_amount\n",
    "\n",
    "    # get current frame\n",
    "    if use_ema_tensor:\n",
    "        out = synth( z.average )\n",
    "    else:\n",
    "        out = synth( z.tensor )\n",
    "    pil = TF.to_pil_image(out[0].cpu())\n",
    "\n",
    "    # do the transformations\n",
    "    sx, sy = pil.size\n",
    "    im1 = pil.resize((sx + zoom, sy + zoom), Image.LANCZOS)\n",
    "    im1 = im1.rotate(rotate_amount)\n",
    "    pil.paste(im1, (-zoom//2, -zoom//2))\n",
    "\n",
    "    #re-encode\n",
    "    z, *_ = model.encode(TF.to_tensor(pil).to(device).unsqueeze(0) * 2 - 1)\n",
    "    z = EMATensor(z, ema_val)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "id": "gmDLxJc4NY4j"
   },
   "outputs": [],
   "source": [
    "#@title  Miscellaneous\n",
    "#@markdown These are some extra parameters you can tweak to change how some of the underlying systems operate during the generation. Only change if you know what you are doing!\n",
    "\n",
    "resume_from_last = False #@param {type:'boolean'}\n",
    "#@markdown - Use the last image generated as the init image for the next run\n",
    "\n",
    "save_params_to_exif = False #@param {type:'boolean'}\n",
    "#@markdown - Save run parameters into the metadata of the png file\n",
    "\n",
    "#multi_clip_should_match_cuts = True #@param {type:'boolean'}\n",
    "\n",
    "cut_weight_pow = 0.5 #@param {type:'number'}\n",
    "#@markdown - The power the length * width of the cut is raised to to calculate its prompt loss weight. 1.0 would make the weight be based on area. 0.5 is based on a square leg. 0 would disable.\n",
    "\n",
    "normalize_prompt_weights = True #@param {type:'boolean'}\n",
    "#@markdown - Should the total weight of the text prompts stay in the same range, relative to other loss functions?\n",
    "\n",
    "use_ema_tensor = False #@param {type:'boolean'}\n",
    "#@markdown - Enabling the EMA tensor will cause the image to be slower to generate but may help it be more cohesive.\n",
    "#@markdown This can also help keep the final image closer to the init image, if you are providing one.\n",
    "\n",
    "### MSE Loss Behavior\n",
    "mse_epoch_step = 50 #@param {type:'number'}\n",
    "mse_epoch_count = 5 #@param {type:'number'}\n",
    "\n",
    "\n",
    "\n",
    "advanced_cells_have_been_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "form",
    "id": "YWGR8q9AT8uk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Using device: cuda:0\n",
      "using prompts:  South African Cave Painting of a Fractal\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fc4ee4c33b42c3b73f4e91e866adf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed 8044 for randomized run\n",
      "[{'model': 'ViT-B/32', 'weight': 1.0, 'prompt': 'South African Cave Painting of a Fractal'}]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 282>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=282'>283</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm() \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=283'>284</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m runs \u001b[39m<\u001b[39m run_limit \u001b[39mor\u001b[39;00m run_limit \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=284'>285</a>\u001b[0m         init()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=286'>287</a>\u001b[0m         i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=287'>288</a>\u001b[0m         losses \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb Cell 24'\u001b[0m in \u001b[0;36minit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=136'>137</a>\u001b[0m \u001b[39mprint\u001b[39m( clip_models )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=138'>139</a>\u001b[0m \u001b[39mglobal\u001b[39;00m clip_loss\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=139'>140</a>\u001b[0m clip_loss \u001b[39m=\u001b[39m MultiClipLoss( clip_models, \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=140'>141</a>\u001b[0m     normalize_prompt_weights \u001b[39m=\u001b[39;49m run_args\u001b[39m.\u001b[39;49mnormalize_prompt_weights, \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=141'>142</a>\u001b[0m     cutn \u001b[39m=\u001b[39;49m run_args\u001b[39m.\u001b[39;49mcut_n, \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=142'>143</a>\u001b[0m     cut_weight_pow \u001b[39m=\u001b[39;49m run_args\u001b[39m.\u001b[39;49mcut_weight_pow)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=144'>145</a>\u001b[0m \u001b[39m# reset seed before making init image\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=145'>146</a>\u001b[0m \u001b[39m# (this should already be set to something other than -1 from calling randomize_run)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000023?line=146'>147</a>\u001b[0m run_args\u001b[39m.\u001b[39mgen_seed \u001b[39m=\u001b[39m update_random( run_args\u001b[39m.\u001b[39mgen_seed, \u001b[39m'\u001b[39m\u001b[39mimage generation\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb Cell 9'\u001b[0m in \u001b[0;36mMultiClipLoss.__init__\u001b[0;34m(self, clip_models, normalize_prompt_weights, cutn, cut_weight_pow, clip_weight)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000008?line=88'>89</a>\u001b[0m \u001b[39mfor\u001b[39;00m pw \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperceptor_work:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000008?line=89'>90</a>\u001b[0m     p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperceptors[ pw[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000008?line=90'>91</a>\u001b[0m     pw[\u001b[39m'\u001b[39m\u001b[39membeds\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_prompt_embeds( p[\u001b[39m'\u001b[39;49m\u001b[39mclip\u001b[39;49m\u001b[39m'\u001b[39;49m], pw[\u001b[39m'\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m'\u001b[39;49m] )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000008?line=91'>92</a>\u001b[0m     pw[\u001b[39m'\u001b[39m\u001b[39mres\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m p[\u001b[39m'\u001b[39m\u001b[39mres\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000008?line=93'>94</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperceptor_work\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m e: e[\u001b[39m'\u001b[39m\u001b[39mres\u001b[39m\u001b[39m'\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb Cell 9'\u001b[0m in \u001b[0;36mMultiClipLoss.get_prompt_embeds\u001b[0;34m(self, perceptor, text_prompt)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000008?line=145'>146</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_prompt_weights \u001b[39mand\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m prompts_weight_sum:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000008?line=146'>147</a>\u001b[0m     weight \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m prompts_weight_sum\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000008?line=148'>149</a>\u001b[0m embed \u001b[39m=\u001b[39m perceptor\u001b[39m.\u001b[39;49mencode_text(clip_token)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000008?line=149'>150</a>\u001b[0m embed_normed \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnormalize(embed\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/c/Desktop/dev/FromScratchSlemVQGan/SlemVQGan.ipynb#ch0000008?line=150'>151</a>\u001b[0m prompt_embeds\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39membed_normed\u001b[39m\u001b[39m'\u001b[39m:embed_normed,\u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m:torch\u001b[39m.\u001b[39mas_tensor(weight, device\u001b[39m=\u001b[39mdevice),\u001b[39m'\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m'\u001b[39m:torch\u001b[39m.\u001b[39mas_tensor(stop, device\u001b[39m=\u001b[39mdevice)})\n",
      "File \u001b[0;32m~/Desktop/dev/FromScratchSlemVQGan/CLIP/clip/model.py:345\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    <a href='file:///home/c/Desktop/dev/FromScratchSlemVQGan/CLIP/clip/model.py?line=343'>344</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_text\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m--> <a href='file:///home/c/Desktop/dev/FromScratchSlemVQGan/CLIP/clip/model.py?line=344'>345</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken_embedding(text)\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)  \u001b[39m# [batch_size, n_ctx, d_model]\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/c/Desktop/dev/FromScratchSlemVQGan/CLIP/clip/model.py?line=346'>347</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    <a href='file:///home/c/Desktop/dev/FromScratchSlemVQGan/CLIP/clip/model.py?line=347'>348</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py?line=156'>157</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py?line=157'>158</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py?line=158'>159</a>\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py?line=159'>160</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2183\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/functional.py?line=2176'>2177</a>\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/functional.py?line=2177'>2178</a>\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/functional.py?line=2178'>2179</a>\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/functional.py?line=2179'>2180</a>\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/functional.py?line=2180'>2181</a>\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/functional.py?line=2181'>2182</a>\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> <a href='file:///home/c/.local/lib/python3.10/site-packages/torch/nn/functional.py?line=2182'>2183</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title ###  Inspiration\n",
    "#@markdown  What do you want to see?\n",
    "#@markdown  \n",
    "\n",
    "try: advanced_cells_have_been_run \n",
    "except NameError: advanced_cells_have_been_run = False\n",
    "if not advanced_cells_have_been_run:\n",
    "    print( \"ERROR: Before you can generate art, you need to run all the cells above this cell first -- including the cells in the Advanced Features section.\" )\n",
    "    raise StopExecution\n",
    "\n",
    "\n",
    "text_prompt = 'South African Cave Painting of a Fractal'#@param {type:'string'}\n",
    "gen_seed = -1#@param {type:'number'}\n",
    "save_art_output = True #@param {type:'boolean'}\n",
    "\n",
    "#@markdown ###  Initialization\n",
    "#@markdown You can use an init image to achieve some more control. The init image can be a url or you can upload it to colab and just include a filename. Try using images from previous runs with new prompts to better direct and morph your art!\n",
    "init_image = ''#@param {type:'string'}\n",
    "width = 800#@param {type:'number'}\n",
    "height = 600#@param {type:'number'}\n",
    "max_iter = 500#@param {type:'number'}\n",
    "\n",
    "#@markdown There are different ways of generating the random starting point, when not using an init image. These influence how the image turns out. The default VQGAN ZRand is good, but some models and subjects may do better with perlin or pyramid noise.\n",
    "#@markdown - If you want to keep starting from the same point, set `gen_seed` to a positive number. `-1` will make it random every time. \n",
    "rand_init_mode = 'VQGAN ZRand'#@param [ \"VQGAN ZRand\", \"Perlin Noise\", \"Pyramid Noise\"]\n",
    "perlin_octaves = 2#@param {type:\"slider\", min:1, max:8, step:1}\n",
    "perlin_weight = 0.22#@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "pyramid_octaves = 5#@param {type:\"slider\", min:1, max:8, step:1}\n",
    "pyramid_decay = 0.99#@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "ema_val = 0.99\n",
    "\n",
    "#@markdown ###  Generation\n",
    "#@markdown Picking a different VQGAN model will impact how an image generates. Think of this as giving the generator a different set of brushes and paints to work with. CLIP is still the \"eyes\" and is judging the image against your prompt but using different brushes will make a different image.\n",
    "#@markdown - `vqgan_imagenet_f16_16384` is the default and what most people use\n",
    "vqgan_model = 'wikiart_16384'#@param [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
    "\n",
    "#@markdown ### Perception\n",
    "#@markdown How many slices of the image should be sent to CLIP each iteration to score? Higher numbers are better, but cost more memory. If you are running into memory issues try lowering this value.\n",
    "cut_n =  128#@param {type:'number'}\n",
    "\n",
    "#@markdown One clip model is good. Two is better? You may need to reduce the number of cuts to support having more than one CLIP model. CLIP is what scores the image against your prompt and each model has slightly different ideas of what things are.\n",
    "#@markdown - `ViT-B/32` is fast and good and what most people use to begin with\n",
    "\n",
    "clip_model = 'ViT-B/32' #@param [\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\",\"RN50\",\"RN101\",\"RN50x64\"]\n",
    "clip_model2 ='None' #@param [\"None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\", \"RN50\",\"RN101\",\"RN50x64\"]\n",
    "if clip_model2 == \"None\":\n",
    "    clip_model2 = None \n",
    "clip1_weight = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "\n",
    "#@markdown ###  Learning Rates & Weights\n",
    "#@markdown Learning rates greatly impact how quickly an image can generate, or if an image can generate at all. The first learning rate is only for the first 50 iterations. The epoch rate is what is used after reaching the first mse epoch. \n",
    "#@markdown You can try lowering the epoch rate while raising the initial learning rate and see what happens\n",
    "learning_rate = 0.2#@param {type:'number'}\n",
    "learning_rate_epoch = 0.2#@param {type:'number'}\n",
    "#@markdown How much should we try to match the init image, or if no init image how much should we resist change after reaching the first epoch?\n",
    "mse_weight = 0.5 #@param {type:'number'}\n",
    "#@markdown Adding some TV may make the image blurrier but also helps to get rid of noise. A good value to try might be 0.1.\n",
    "tv_weight = 0.0 #@param {type:'number'}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown  <font size=\"5\"></font> I'd love to see what you can make with my notebook. Tweet me your art [@remi_durant](https://twitter.com/remi_durant)!\n",
    "\n",
    "output_as_png = True\n",
    "\n",
    "run_args = argparse.Namespace(\n",
    "    base_prompt = text_prompt,\n",
    "    text_prompt = text_prompt,\n",
    "    clip_model = clip_model,\n",
    "    clip_model2 = clip_model2,\n",
    "    clip1_weight = clip1_weight,\n",
    "    gen_seed = gen_seed,\n",
    "    #clip_models = clip_models,\n",
    "    width = width,\n",
    "    height = height,\n",
    "    rand_init_mode = rand_init_mode,\n",
    "    perlin_octaves = perlin_octaves,\n",
    "    perlin_weight = perlin_weight,\n",
    "    pyramid_octaves = pyramid_octaves,\n",
    "    pyramid_decay = pyramid_decay,\n",
    "    cut_n = cut_n,\n",
    "    cut_weight_pow = cut_weight_pow,\n",
    "    learning_rate = learning_rate,\n",
    "    learning_rate_epoch = learning_rate_epoch,\n",
    "    mse_weight = mse_weight,\n",
    "    mse_epoch_step = mse_epoch_step,\n",
    "    mse_epoch_count = mse_epoch_count,\n",
    "    tv_weight = tv_weight,\n",
    "    normalize_prompt_weights = normalize_prompt_weights,\n",
    "    vqgan_model = vqgan_model,\n",
    "\n",
    "    noise_frac_base = noise_frac_base,\n",
    ")\n",
    "\n",
    "if DoNoiseBurst:\n",
    "    run_args.noise_burst_start = noise_burst_start\n",
    "    run_args.noise_burst_end = noise_burst_end\n",
    "    run_args.noise_burst_freq = noise_burst_freq\n",
    "    run_args.noise_burst_length = noise_burst_length\n",
    "    run_args.noise_frac_burst = noise_frac_burst\n",
    "\n",
    "print('Using device:', device)\n",
    "print('using prompts: ', text_prompt)\n",
    "\n",
    "MultiClipLoss.perceptors = {} # remove any preloaded clip models\n",
    "clear_memory()\n",
    "\n",
    "model = get_vqgan_model( vqgan_model )\n",
    "\n",
    "losses = []\n",
    "mb = master_bar(range(1))\n",
    "gnames = ['losses']\n",
    "\n",
    "mb.names=gnames\n",
    "mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
    "mb.graph_ax = axs\n",
    "mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
    "\n",
    "## init step\n",
    "def init():    \n",
    "    randomize_run()\n",
    "\n",
    "    BuildRunRecord()\n",
    "\n",
    "    if run_args.clip_model2:     \n",
    "        clip_models = [\n",
    "            {'model':run_args.clip_model, 'weight':run_args.clip1_weight, 'prompt':run_args.text_prompt},\n",
    "            {'model':run_args.clip_model2, 'weight':1.0 - run_args.clip1_weight, 'prompt':run_args.text_prompt},\n",
    "        ]\n",
    "    else:\n",
    "        clip_models = [\n",
    "            {'model':run_args.clip_model, 'weight':1.0, 'prompt':run_args.text_prompt},\n",
    "        ]\n",
    "\n",
    "    print( clip_models )\n",
    "\n",
    "    global clip_loss\n",
    "    clip_loss = MultiClipLoss( clip_models, \n",
    "        normalize_prompt_weights = run_args.normalize_prompt_weights, \n",
    "        cutn = run_args.cut_n, \n",
    "        cut_weight_pow = run_args.cut_weight_pow)\n",
    "\n",
    "    # reset seed before making init image\n",
    "    # (this should already be set to something other than -1 from calling randomize_run)\n",
    "    run_args.gen_seed = update_random( run_args.gen_seed, 'image generation')\n",
    "        \n",
    "    # Make Z Init\n",
    "    global z\n",
    "    z = 0\n",
    "\n",
    "    f = 2**(model.decoder.num_resolutions - 1)\n",
    "    toksX, toksY = math.ceil( run_args.width / f), math.ceil( run_args.height / f)\n",
    "\n",
    "    print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
    "    run_args.width = toksX * f\n",
    "    run_args.height = toksY * f\n",
    "    \n",
    "    global init_image\n",
    "    \n",
    "    if resume_from_last and last_image_saved is not None:\n",
    "        print(\"Resuming from last generation\")\n",
    "        init_image = last_image_saved\n",
    "\n",
    "    has_init_image = (init_image != \"\")\n",
    "    if has_init_image:\n",
    "        if 'http' in init_image:\n",
    "            req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            img = Image.open(urlopen(req))\n",
    "        else:\n",
    "            img = Image.open(init_image)\n",
    "\n",
    "        pil_image = img.convert('RGB')\n",
    "        pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
    "        pil_image = TF.to_tensor(pil_image)\n",
    "        #if args.use_noise:\n",
    "        #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
    "        z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
    "        del pil_image\n",
    "        del img\n",
    "\n",
    "    else:\n",
    "        z = make_rand_init( run_args.rand_init_mode, model, run_args.perlin_octaves, run_args.perlin_weight, run_args.pyramid_octaves, run_args.pyramid_decay, toksX, toksY, f )\n",
    "        \n",
    "    z = EMATensor(z, ema_val)\n",
    "    \n",
    "    global opt\n",
    "    opt = optim.Adam( z.parameters(), lr=run_args.learning_rate, weight_decay=0.00000000)\n",
    "\n",
    "    global mse_loss\n",
    "    mse_loss = MSEDecayLoss( run_args.mse_weight, \n",
    "                            mse_decay_rate = mse_epoch_step, \n",
    "                            mse_epoches = mse_epoch_count, \n",
    "                            mse_quantize=True )\n",
    "    mse_loss.set_target( z.tensor, model )\n",
    "    mse_loss.has_init_image = has_init_image\n",
    "\n",
    "    global tv_loss\n",
    "    tv_loss = TVLoss() \n",
    "\n",
    "## optimizer loop\n",
    "\n",
    "def synth(z, quantize=True, scramble=True):\n",
    "    z_q = 0\n",
    "    if quantize:\n",
    "      z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
    "    else:\n",
    "      z_q = z.model\n",
    "\n",
    "    out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
    "\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def checkin(i, z, out_pil, losses):\n",
    "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
    "\n",
    "    print( 'Prompt:', run_args.text_prompt )\n",
    "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
    "\n",
    "    display_format='png' if output_as_png else 'jpg'\n",
    "    pil_data = image_to_data_url(out_pil, display_format)\n",
    "    \n",
    "    display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
    "\n",
    "def train(i):\n",
    "    global opt\n",
    "    global z \n",
    "    opt.zero_grad( set_to_none = True )\n",
    "\n",
    "    out = checkpoint( synth, z.tensor )\n",
    "\n",
    "    clip_loss.noise_aug.frac = get_noise_frac(i)\n",
    "\n",
    "    lossAll = []\n",
    "    lossAll += clip_loss( i,out )\n",
    "\n",
    "    if 0 < run_args.mse_weight:\n",
    "        msel = mse_loss(i,z.tensor)\n",
    "        if 0 < msel:\n",
    "            lossAll.append(msel)\n",
    "    \n",
    "    if 0 < run_args.tv_weight:\n",
    "        lossAll.append(tv_loss(out)*run_args.tv_weight)\n",
    "    \n",
    "    loss = sum(lossAll)\n",
    "    loss.backward()\n",
    "\n",
    "    if should_checkin(i) or should_save_for_video(i):\n",
    "        with torch.no_grad():\n",
    "            if use_ema_tensor:\n",
    "                out = synth( z.average )\n",
    "\n",
    "            pil = TF.to_pil_image(out[0].cpu())\n",
    "\n",
    "            if should_checkin(i):\n",
    "                if not headless:\n",
    "                    checkin(i, z, pil, lossAll)\n",
    "                SaveRunIteration( i, pil )\n",
    "            \n",
    "            if should_save_for_video(i):\n",
    "                save_frame_for_video(i, pil)   \n",
    "\n",
    "    # update graph\n",
    "    losses.append(loss.item())\n",
    "    x = range(len(losses))\n",
    "    mb.update_graph( [[x,losses]] )\n",
    "\n",
    "    opt.step()\n",
    "    if use_ema_tensor:\n",
    "      z.update()\n",
    "\n",
    "set_drive_output_subfolder()\n",
    "\n",
    "run_limit = 1\n",
    "if BatchRunOverride:\n",
    "    run_limit = batch_run_limit\n",
    "\n",
    "from IPython.display import clear_output \n",
    "runs = 0\n",
    "try:\n",
    "    with tqdm() as pbar:\n",
    "        while runs < run_limit or run_limit == -1:\n",
    "            init()\n",
    "\n",
    "            i = 0\n",
    "            losses = []\n",
    "\n",
    "            # clear out folder for video frames\n",
    "            if save_frames_for_video:                                \n",
    "                !rm -r steps\n",
    "                !mkdir -p steps\n",
    "            \n",
    "            while True and i <= max_iter:\n",
    "        \n",
    "                if i % 200 == 0:\n",
    "                    clear_memory()\n",
    "\n",
    "                train(i)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    should_do_mse_epoch = mse_loss.step(i)\n",
    "                    if should_do_mse_epoch or should_do_animation_step(i):\n",
    "                        \n",
    "                        if should_do_mse_epoch:\n",
    "                            print(f'Reseting optimizer at mse epoch (i: {i})')\n",
    "\n",
    "                            if mse_loss.has_init_image and use_ema_tensor:\n",
    "                                mse_loss.set_target(z.average,model)\n",
    "                            else:\n",
    "                                mse_loss.set_target(z.tensor,model)                            \n",
    "                             \n",
    "                            # Make sure not to spike loss when mse_loss turns on\n",
    "                            if not mse_loss.is_active(i):\n",
    "                                z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
    "                                z.tensor.requires_grad = True\n",
    "                            \n",
    "                        if should_do_animation_step(i):\n",
    "                            print(f'Applying animation step and reseting optimizer (i: {i})')\n",
    "                            do_anim_step()\n",
    "\n",
    "                            mse_loss.set_target(z.tensor,model)\n",
    "\n",
    "                        if use_ema_tensor:\n",
    "                            z = EMATensor(z.average, ema_val)\n",
    "                        else:\n",
    "                            z = EMATensor(z.tensor, ema_val)\n",
    "                        \n",
    "                        opt = optim.Adam(z.parameters(), lr=run_args.learning_rate_epoch, weight_decay=0.00000000)\n",
    "\n",
    "                # step completed\n",
    "                if not BatchRunOverride:\n",
    "                    pbar.update()\n",
    "                i += 1\n",
    "                \n",
    "            if save_frames_for_video and autogenerate_video_after_run:\n",
    "                save_video()\n",
    "\n",
    "            # image completed\n",
    "            if BatchRunOverride:\n",
    "                pbar.update()\n",
    "            runs += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Iaw63lWBqsnj"
   },
   "outputs": [],
   "source": [
    "#@title  Make a Video of Your Last Run!\n",
    "#@markdown If you want to make a video, you must first enable `save_frames_for_video` in the Advanced section.\n",
    "\n",
    "save_video()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
